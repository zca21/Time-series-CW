---
title: "Untitled"
output: html_document
date: '2022-04-23'
---
```{r echo=TRUE}
#************************#
#!!! VERY IMPORTANT !!!
#************************#
# Please, replace the "2" inside set.seed() with your
# unique seed
set.seed(74830)

#************************#
#!!! VERY IMPORTANT !!!
#
#  DON'T MODIFY THE LINES
#  IN THE REMAINING OF 
#  THIS R CHUNK
#
#************************#
# Loading data
load("lts0.Rda")

# Extracting time series
idx1 <- sample(1:500,size=4,replace=FALSE)
idx2 <- sample(501:1000,size=3,replace=FALSE)
idx3 <- sample(1001:1500,size=3,replace=FALSE)
tser01 <- lts0[[idx1[1]]]
tser02 <- lts0[[idx1[2]]]
tser03 <- lts0[[idx1[3]]]
tser04 <- lts0[[idx1[4]]]
tser05 <- lts0[[idx2[1]]]
tser06 <- lts0[[idx2[2]]]
tser07 <- lts0[[idx2[3]]]
tser08 <- lts0[[idx3[1]]]
tser09 <- lts0[[idx3[2]]]
tser10 <- lts0[[idx3[3]]]

# Test you've got the time series in the workspace
par(mfrow=c(2,2))
plot(tser01)
plot(tser02)
plot(tser03)
plot(tser04)
par(mfrow=c(3,1))
plot(tser05)
plot(tser06)
plot(tser07)
par(mfrow=c(3,1))
plot(tser08)
plot(tser09)
plot(tser10)

# Back to one plot per window
par(mfrow=c(1,1))
```

**SOLUTION**


```{r}
#setup chunk 
library(astsa)
library(tseries)

#Initial checks, stationary?- KS test, stl- seasonal/trend component - elimiate (differencing or repeated differencing), [note dont't want to over difference]

#not that pacf starts at lag 1 while acf starts at lag 0

#can add in curves on AR 1 processes to back up selected model (W23 arima simulations)
```


## TS 1-4


```{r}
#To initially investigate what ARIMA models ACF/PACF with d=1 would look like create some references

arima111 <- arima.sim(n=10000,model=list(order=c(1,1,1),ma=c(0.8),ar=c(0.5)),sd=1)
acf(arima111)
pacf(arima111)

arima211 <- arima.sim(n=10000,model=list(order=c(2,1,1),ma=c(0.6),ar=c(0.5,0.3)),sd=1)
acf(arima211)
pacf(arima211)

arima212 <- arima.sim(n=10000,model=list(order=c(2,1,2),ma=c(0.8,0.2),ar=c(0.5,0.3)),sd=1)
acf(arima212)
pacf(arima212)
# from above plots ius apparent that due to non stationarity of process when d=1 the correlgram has non decaying spikes for tau=integers thus can use this to identify time series with d=1 in first 4 time series


#adf.test(arima212)

#perform normal dickey-fuller test (k=0) as only want to know if 1st difference is unit root (as told d=0 or 1)
#p-value significant so reject null hypothesis of the presence of a unit root and therefore the series is stationary (ie d=0 as if d=1 the process is non-stationary)
```


#TS 1
```{r}
#Start by plotting the 1st time series (of 300 obs)
plot(tser01)

#initial observation of the ts object: it doesn't look to have a trend or seasonal component, the variation doesn't appear to change as time increased thus appears the ts object is not heteroscedastic

#ks test, spit in half to examine hetroscedasticity
x<-tser01[1:150]
y<-tser01[151:300]
ks.test(x,y)
#very high p-value backing up our original claims from observation that ts is not heteroscedastic and no trend present therefore is stationary

#1. Identification of likely p,q,d values (use final differenceing to take to make stationary- to give d)

acf(tser01)
#only correlated at lag equal to zero 
pacf(tser01)
#partial not significant (no partial correlation)

#initial analysis suggests object is white noise as only significant spike of acf is at tau=0 and pacf has no significant spikes at any lag
#thus try fitting a ARIMA(0,0,0) model

fit_ts1<-arima(tser01,order=c(0,0,0))
print(fit_ts1)

#no point in creating CI as no alpha or beta to estimate

#further dignostics
tsdiag(fit_ts1)

#no trends/patterns observed in the residuals
#no evidence of autocorrelation in the residuals
#consistently high LB p-values
#therefore no evidence that theres still some time structure we haven't modelled

#final check 
lag.plot(resid(fit_ts1),do.lines=FALSE)
#observe a globular cloud of points, thus data uncorrelated 

#thus conclude that the time series is Gaussian random noise ie ARIMA(0,0,0) (white noise process)
```

#TS 2
```{r}
#Start by plotting the time series
plot(tser02)
#By observation of the TS I note that there does not appear to be any trend, seaosnality or heteroscedasicity

#ks test, spit in half to examine hetroscedasticity
x<-tser02[1:150]
y<-tser02[151:300]
ks.test(x,y)
#no heteroscedasity from kolomogov-smearnov test

#1. Identification of likely p,q,d values
#use correlograms 
par(mfrow=c(2,1))
acf(tser02)
pacf(tser02)
#acf cuts off at 3 and pacf decays exponentially indicating a MA(2) process

#2. Parameter esitmation 
#I fit a model using the the p,q,d values above (00,2) to estimate the parameters (beta)
fit_ts2<- arima(tser02,order=c(0,0,2))
print(fit_ts2)
#creating 95% CI for beta 1
se_1<- sqrt(fit_ts2$var.coef[1,1])
print(c(fit_ts2$coef[1]-2*se_1,fit_ts2$coef[1]+2*se_1))
#beta 1 significantly different from 0

#beta 2 CI
se_2<- sqrt(fit_ts2$var.coef[2,2])
print(c(fit_ts2$coef[2]-2*se_2,fit_ts2$coef[2]+2*se_2))
#beta significantly different from 0 so 95% sure

#further dignostics
tsdiag(fit_ts2) 
#note that p-values of 2,3,4 and 5 are significant in Ljung-Box statistic which gives evidence that our model may be incorrect

#Thus I create a new model assuming model looking back at the acf and pacf I note that perhaps the ACF is exhibiting sinisudal dampening and thus we have a ARMA process, I start with a p=1 and q=1 process model 


fit_ts2_2<- arima(tser02,order=c(1,0,1))
print(fit_ts2_2)

#creating 95% CI for alpha_1
se_1 <- sqrt(fit_ts2_2$var.coef[1,1])
print(c(fit_ts2_2$coef[1]-2*se_1,fit_ts2_2$coef[1]+2*se_1))
#alpha significantly different from 0

#CI for beta_1
se_2 <- sqrt(fit_ts2_2$var.coef[2,2])
print(c(fit_ts2_2$coef[2]-2*se_2,fit_ts2_2$coef[2]+2*se_2))
#alpha significantly different from 0

tsdiag(fit_ts2_2)
#diagnostics look good: no pattern in residuals, no autocorrelation of residuals and LB p-values all high

#creating several higher order models to compare against p=1 and q=1 model

#try a p=1, q=2 model
fit_ts2_3<- arima(tser02,order=c(1,0,2))
#creating 95% CI for alpha/beta's
#alpha_1 CI
se_1 <- sqrt(fit_ts2_3$var.coef[1,1])
print(c(fit_ts2_3$coef[1]-2*se_1,fit_ts2_3$coef[1]+2*se_1))
#significantly different from 0

#beta_1 CI
se_2 <- sqrt(fit_ts2_3$var.coef[2,2])
print(c(fit_ts2_3$coef[2]-2*se_2,fit_ts2_3$coef[2]+2*se_2))
#significantly different from 0

#beta_2 CI
se_3 <- sqrt(fit_ts2_3$var.coef[3,3])
print(c(fit_ts2_3$coef[3]-2*se_3,fit_ts2_3$coef[3]+2*se_3))
#not significantly different from 0 as CI includes 0 thus TS shouldn't have q=2


# p=2, q=1 model
fit_ts2_4<- arima(tser02,order=c(2,0,1))

#creating CI's
#creating 95% CI for alpha_1
se_1 <- sqrt(fit_ts2_4$var.coef[1,1])
print(c(fit_ts2_4$coef[1]-2*se_1,fit_ts2_4$coef[1]+2*se_1))
#alpha significantly different from 0

#creating 95% CI for alpha_2
se_2 <- sqrt(fit_ts2_4$var.coef[2,2])
print(c(fit_ts2_4$coef[2]-2*se_2,fit_ts2_4$coef[2]+2*se_2))
#alpha 2 not significantly different from 0 thus p=1 not 2 so reject this model

#final look at residuals of ARIMA(1,0,1) model
lag.plot(resid(fit_ts2_2),do.lines=FALSE)
#globular smattering of points thus white noise


#As higher order models have non significant higher order terms conclude that ARIMA(1,0,1) is the correct model with a alpha_1 estimated to be -0.5 and beta_1 to be -0.6 (1.d.p.)
```

##TS 3
```{r}
plot(tser03)

x<-tser03[1:150]
y<-tser03[151:300]
ks.test(x,y)
#no hetroscedasity or trend observed (as p-value high)

#1. Identification of likely p,q,d values

#plotting correlograms 
acf(tser03)
#cuts off after lag of 2
pacf(tser03)
#could be decaying however lag increases from 1 to 2 
#initially fit a MA(2) model to the data

fit_ts3<- arima(tser03,order=c(0,0,2))
print(fit_ts3)
#creating 95% CI for beta 1
se_1<- sqrt(fit_ts3$var.coef[1,1])
print(c(fit_ts3$coef[1]-2*se_1,fit_ts3$coef[1]+2*se_1))
#beta 1 significantly different from 0

#beta 2 CI
se_2<- sqrt(fit_ts3$var.coef[2,2])
print(c(fit_ts3$coef[2]-2*se_2,fit_ts3$coef[2]+2*se_2))
#beta 2 not different from 0 as included in the 95% CI (evidence q=1 instead)

#further dignostics
tsdiag(fit_ts3) 
#also note that the residuals could be correlated as value at lag 2 is significant and p-values at lag 2 and greater are also significant 

#thus conclude current model is incorrect and needs to be updated
#assume ARMA model and start with p=1 and q=1
fit_ts3_2<- arima(tser03,order=c(1,0,1))
tsdiag(fit_ts3_2)
#model still incorrect with autocorrelation at lag 2 

#try p=2, q=1
fit_ts3_3<- arima(tser03,order=c(2,0,1))
tsdiag(fit_ts3_3)
#all diagnostics look good (no patterns in residuals or autocorrelation) and LB test has high p-values

print(fit_ts3_3)
#create CI's
#CI for alpha 1
se_1<- sqrt(fit_ts3_3$var.coef[1,1])
print(c(fit_ts3_3$coef[1]-2*se_1,fit_ts3_3$coef[1]+2*se_1))
#alpha 1 not significantly different from 0

#CI for alpha 2
se_2<- sqrt(fit_ts3_3$var.coef[2,2])
print(c(fit_ts3_3$coef[2]-2*se_2,fit_ts3_3$coef[2]+2*se_2))
#alpha 2 significantly different from 0

#CI for beta 1
se_3<- sqrt(fit_ts3_3$var.coef[3,3])
print(c(fit_ts3_3$coef[3]-2*se_3,fit_ts3_3$coef[3]+2*se_3))
#beta 1 significantly different from 0

#now also create a p=2, q=2 model to compare against
fit_ts3_4<-arima(tser03,order=c(2,0,2))
#check if beta_2 term is significant
#CI for beta 2
se_4<- sqrt(fit_ts3_4$var.coef[4,4])
print(c(fit_ts3_4$coef[4]-2*se_4,fit_ts3_4$coef[4]+2*se_4))
#not significantly different from 0 so reject this model

lag.plot(resid(fit_ts3_3),do.lines=FALSE)
#globualr smattering of residuals

#conclude time series is a ARIMA(2,0,1) process with alpha_1=0, alpha_2=-0.3 and beta_1=-0.7
```

## TS 4
```{r}
plot(tser04)

x<-tser04[1:150]
y<-tser04[151:300]
ks.test(x,y)
#no hetroscedasity or trend observed (as p-value high)

#correlograms
acf(tser04)
#cuts off after lag 2
pacf(tser04)
#sinesdiual dampening perhaps?

#try a MA(2) process to start with
fit_ts4<- arima(tser04,order=c(0,0,2))

tsdiag(fit_ts4)
#diagnostics look good

#construct CI's
#CI for beta_1
se_1<- sqrt(fit_ts4$var.coef[1,1])
print(c(fit_ts4$coef[1]-2*se_1,fit_ts4$coef[1]+2*se_1))
#significantly different from 0

#CI for beta_2
se_2<- sqrt(fit_ts4$var.coef[2,2])
print(c(fit_ts4$coef[2]-2*se_2,fit_ts4$coef[2]+2*se_2))
#significantly different from 0

lag.plot(resid(fit_ts4),do.lines=FALSE)
#globular smattering

print(fit_ts4)
#conclude TS is a ARMIA(0,0,2) process with beta_1=-0.3 and beta_2=0.2
```



# TS 5-7
Note can try and plot curve ontop of the ts to show have correct period (in simulations and autocorrelations)

## TS 5 need to update (SD too high)
```{r}
plot(tser05)
#evidence of seasonality and trend that need to be removed to analysis the stationary component
#288 observations in ts and 6 periods implies 6 years thus number of obs per month (nfreq) is 4

#adding seasonality to the ts object
ts5<-ts(tser05[1:288],start=0,end=(6-1/48),frequency=48)
#stl decomposition
plot(stl(ts5,s.window="periodic"))
#the seasonality appears to of been captured well in the stl plot

#to remove the trend and seasonality we take a difference at lag 48 (the period of the ts) and at lag 1 to remove the trend
t_ts5<-diff(diff(tser05),lag=48)
plot(t_ts5)

acf(t_ts5)
acf(t_ts5,lag.max=300)
#most of the seasonality and trend appear to of been removed
#ts appears stationary now so can begin analysis of ts

pacf(t_ts5)
#acf cuts off after lag 2 and pacf decays sinasudally, so propose using MA(2)

#try a MA(2) process to start with
fit_ts5<- arima(t_ts5,order=c(0,0,2))

tsdiag(fit_ts5)
#not a good fit

#try a ARMA model instead
#start with p=1, q=1
fit_ts5_2<- arima(t_ts5,order=c(1,0,1))
tsdiag(fit_ts5_2)
#not good fit, autocorrelation of residuals and LB p-values significant for lags 2 and greater

#p=1, q=2 model
fit_ts5_3<- arima(t_ts5,order=c(1,0,2))
print(fit_ts5_3)
#model has standard deviation of 4.08 therefore is out of the bounds

#try p=2, q=2 model
fit_ts5_4<- arima(t_ts5,order=c(2,0,2))
print(fit_ts5_4)
#model standard deviation still out of bounds

#try p=2, q=3 model
fit_ts5_5<- arima(t_ts5,order=c(2,0,3))
print(fit_ts5_5)

fit_ts5_5<- arima(t_ts5,order=c(3,0,3))
print(fit_ts5_5)








#check CI's
#CI for alpha 1
se_1<- sqrt(fit_ts5_3$var.coef[1,1])
print(c(fit_ts5_3$coef[1]-2*se_1,fit_ts5_3$coef[1]+2*se_1))
#alpha 1 significantly different from 0

#CI for beta 1
se_2<- sqrt(fit_ts5_3$var.coef[2,2])
print(c(fit_ts5_3$coef[2]-2*se_2,fit_ts5_3$coef[2]+2*se_2))
#beta 1 significantly different from 0

#CI for beta 2
se_3<- sqrt(fit_ts5_3$var.coef[3,3])
print(c(fit_ts5_3$coef[3]-2*se_3,fit_ts5_3$coef[3]+2*se_3))
#beta 2 significantly different from 0

#check that higher order model p=2, q=2 isn't better
fit_ts5_4<- arima(t_ts5,order=c(2,0,2))
print(fit_ts5_4)
tsdiag(fit_ts5_4)
##check CI's
#CI for alpha 2
se_2<- sqrt(fit_ts5_4$var.coef[2,2])
print(c(fit_ts5_4$coef[2]-2*se_2,fit_ts5_4$coef[2]+2*se_2))
#not significantly different from zero therefore q=1 not 2

lag.plot(resid(fit_ts5_3),do.lines=FALSE)
#globular smattering

print(fit_ts5_3)
#once removed seasonailty and trend the TS is a ARMIA(1,0,2) process with alpha=-0.2 and beta_1=-0.77 abnd beta_2=0.93

sqrt(16.66)
```
## TS 6
```{r}
plot(tser06)
#appears to be trend so take difference
#take differences until obvious trends disappear

plot(diff(tser06))
#still clearly not stationary so take difference again
plot(diff(diff(tser06)))
#appears to be stationary now look at acf to see if stationary

t_ts6<-diff(diff(tser06))
#correograms
acf(t_ts6)
pacf(t_ts6)
#acf cuts off after 1st lag
#pacf appears to be sinesidual decay
#thus appears to be a MA(1) process 

#fit ARIMA(0,0,1) model
fit_ts6<- arima(t_ts6,order=c(0,0,1))
tsdiag(fit_ts6)
# not fit p value low and auto correlations of the residuals

#try p=1, q=1 model
fit_ts6_2<- arima(t_ts6,order=c(1,0,1))
tsdiag(fit_ts6_2)
#fit not good

#try p=1, q=2 model
fit_ts6_3<- arima(t_ts6,order=c(1,0,2))
tsdiag(fit_ts6_3)
#fit looks good
fit_ts6_3$sigma2
#sd of 3 within bounds


#look at CI's of parameters
#CI for alpha 1
se_1<- sqrt(fit_ts6_3$var.coef[1,1])
print(c(fit_ts6_3$coef[1]-2*se_1,fit_ts6_3$coef[1]+2*se_1))
#alpha 1 significantly different from 0

#CI for beta 1
se_2<- sqrt(fit_ts6_3$var.coef[2,2])
print(c(fit_ts6_3$coef[2]-2*se_2,fit_ts6_3$coef[2]+2*se_2))
#beta 1 significantly different from 0

#CI for beta 2
se_3<- sqrt(fit_ts6_3$var.coef[3,3])
print(c(fit_ts6_3$coef[3]-2*se_3,fit_ts6_3$coef[3]+2*se_3))
#beta 2 significantly different from 0

#check against higher order models
#p=2, q=2 model
fit_ts6_4<- arima(t_ts6,order=c(2,0,2))
tsdiag(fit_ts6_4)
print(fit_ts6_4)
#sd within bounds and model diagnostics good

#CI for alpha 2
se_2<- sqrt(fit_ts6_4$var.coef[2,2])
print(c(fit_ts6_4$coef[2]-2*se_2,fit_ts6_4$coef[2]+2*se_2))
#not significantly different from zero so ignore higher order AR terms

#p=1, q=3 model
fit_ts6_5<- arima(t_ts6,order=c(1,0,3))
tsdiag(fit_ts6_5)
print(fit_ts6_5)
#sigma within range and model diagnostics good

#check CI for beta 3
se_4<- sqrt(fit_ts6_5$var.coef[4,4])
print(c(fit_ts6_5$coef[4]-2*se_4,fit_ts6_5$coef[4]+2*se_4))
#CI includes zero therefore no third beta term in model

#thus conlucde that ts is a ARIMA(1,1,2) (as had to take 2 differences) with no seasonailty component but a trend component
```


## TS 7

```{r}
library(tidyverse)
tser07%>%
  ts(freq=)
  decompose()
```


```{r}
plot(tser07)
#appears to have seasonality but no trend
#could there be 2 seasonal trends as seems to oscillate up and down in a period of 2

tser07%>%
  ts(freq=48)%>%
  tail(12*4*4)%>%
  decompose()

#might have 4 periods thus 4 observations per month

#adding seasonality to the ts object
ts7<-ts(tser07[1:192],start=0,end=(4-1/48),frequency=48)
#stl decomposition
plot(stl(ts7,s.window="periodic"))

#assuming 3.5 periods?

plot(diff(tser07,lag = 48))



```
## TS 8
```{r}
plot(tser08)
#appears to be a polynomial trend (take difference twice to remove) and may be some seasonality in it

#take 1st difference
plot(diff(tser08))
#appears to of removed the trend (check with acf toi make sure trend wasn't higher order polynomial)
acf(diff(tser08))
#no slow decaying spikes so trend appears to of been removed (sinisudal decay to zero)
pacf(diff(tser08))
#only a spike at one cuts off 
#thus appears after taking difference we have an AR(1) process

#fit the model
t_ts8<-diff(tser08)
fit_ts8<- arima(t_ts8,order=c(1,0,0))
tsdiag(fit_ts8)
#all plots look good
print(fit_ts8)
#SD of 1 so within range

#check CI's
#CI for alpha 1
se_1<- sqrt(fit_ts8$var.coef[1,1])
print(c(fit_ts8$coef[1]-2*se_1,fit_ts8$coef[1]+2*se_1))
#alpha 1 significantly different from 0

#check higher order models
fit_ts8_2<- arima(t_ts8,order=c(1,0,1))
tsdiag(fit_ts8_2)
print(fit_ts8_2)
#CI for beta 1
se_1<- sqrt(fit_ts8_2$var.coef[2,2])
print(c(fit_ts8_2$coef[2]-2*se_1,fit_ts8_2$coef[2]+2*se_1))


lag.plot(resid(fit_ts8),do.lines=FALSE)
#looks like a globualr smattering so happy with model overall
```
```{r}
plot(tser09)
#appears that it could be a polynomial trend so take difference twice and see if we have a sationary series


acf(diff(tser09))

t_ts9<-diff(diff(tser09))
acf(t_ts9)
#appears to now be a stationary time series
pacf(t_ts9)
#acf cuts off after 0 lag and pacf might be not significant

#try fitting a ARIMA(0,0,0) model

fit_test<-arima(t_ts9,order=c(1,0,2))
tsdiag(fit_test)

fit_test<-arima(t_ts9,order=c(2,0,2))
tsdiag(fit_test)


plot(tser09,ylim=c(0,300),xlim=c(0,40))


```

```{r}
plot(tser10)
#time series appears to be heteroscedastic
x<-tser10[1:50]
y<-tser10[51:100]
ks.test(x,y)
#ks test identifiers it as not heteroscedastic (no seasonal trend and from same ts)

#does appear to be a regular pattern in the ts with the size increasing and decreasing regularly with 4 total periods

#taking lag of 25 (size of period does appear of the reduced affect of any periodicity)
plot(diff(tser10,lag=25))

#ts does not appear to have any obvious seasonality o

acf(tser10)
pacf(tser10)
```



