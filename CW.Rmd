---
title: "MA30085 Coursework"
output: html_document
date: '2022-04-23'
---
```{r,include=F}
#************************#
#!!! VERY IMPORTANT !!!
#************************#
# Please, replace the "2" inside set.seed() with your
# unique seed
set.seed(74830)

#************************#
#!!! VERY IMPORTANT !!!
#
#  DON'T MODIFY THE LINES
#  IN THE REMAINING OF 
#  THIS R CHUNK
#
#************************#
# Loading data
load("lts0.Rda")

# Extracting time series
idx1 <- sample(1:500,size=4,replace=FALSE)
idx2 <- sample(501:1000,size=3,replace=FALSE)
idx3 <- sample(1001:1500,size=3,replace=FALSE)
tser01 <- lts0[[idx1[1]]]
tser02 <- lts0[[idx1[2]]]
tser03 <- lts0[[idx1[3]]]
tser04 <- lts0[[idx1[4]]]
tser05 <- lts0[[idx2[1]]]
tser06 <- lts0[[idx2[2]]]
tser07 <- lts0[[idx2[3]]]
tser08 <- lts0[[idx3[1]]]
tser09 <- lts0[[idx3[2]]]
tser10 <- lts0[[idx3[3]]]

# Test you've got the time series in the workspace
par(mfrow=c(2,2))
plot(tser01)
plot(tser02)
plot(tser03)
plot(tser04)
par(mfrow=c(3,1))
plot(tser05)
plot(tser06)
plot(tser07)
par(mfrow=c(3,1))
plot(tser08)
plot(tser09)
plot(tser10)

# Back to one plot per window
par(mfrow=c(1,1))
```

**SOLUTION**


```{r}
#setup chunk 
library(astsa)
library(tseries)
```


## TS 1-4


```{r}
#To initially investigate what ARIMA models ACF/PACF with d=1 would look like create some references

arima111 <- arima.sim(n=10000,model=list(order=c(1,1,1),ma=c(0.8),ar=c(0.5)),sd=1)
acf(arima111)
pacf(arima111)

arima211 <- arima.sim(n=10000,model=list(order=c(2,1,1),ma=c(0.6),ar=c(0.5,0.3)),sd=1)
acf(arima211)
pacf(arima211)

arima212 <- arima.sim(n=10000,model=list(order=c(2,1,2),ma=c(0.8,0.2),ar=c(0.5,0.3)),sd=1)
acf(arima212)
pacf(arima212)
# from above plots ius apparent that due to non stationarity of process when d=1 the correlgram has non decaying spikes for tau=integers thus can use this to identify time series with d=1 in first 4 time series


#adf.test(arima212)

#perform normal dickey-fuller test (k=0) as only want to know if 1st difference is unit root (as told d=0 or 1)
#p-value significant so reject null hypothesis of the presence of a unit root and therefore the series is stationary (ie d=0 as if d=1 the process is non-stationary)
```

First I created some simulated ARIMA processes with $d>0$, plotting their ACF and partial ACF correlograms which can be viewed above we see that the acf correlograms spikes do not decrease/ decrease gradually as lag increases. This is due to these time series being non-stationary as proven in lectures. Therefore for these first 4 time series when viewing the acf correlograms I am also checking for non-stationarity (acf correlograms spikes decrease gradually as lag increases) but do not mention this explicitly unless the plot does imply non-stationary of the time series.


#TS 1
```{r}
#Start by plotting the 1st time series (of 300 obs)
plot(tser01)
```
Initially viewing the plot of the first time series object: it doesn't look to have a trend or seasonal component, the variation doesn't appear to change as time increases thus appears the time series object is not heteroscedastic.

```{r}
#ks test, spit in half to examine hetroscedasticity
x<-tser01[1:150]
y<-tser01[151:300]
ks.test(x,y)
#very high p-value backing up our original claims from observation that ts is not heteroscedastic and no trend present therefore is stationary
```
Performing a Kolmogorovâ€“Smirnov test on the first and last half of the time series data gives a very high p-value of 0.997 thus no trend or heteroscedasticity in the time series giving evidence that the process is stationary.

```{r}
#1. Identification of likely p,q,d values (use final differenceing to take to make stationary- to give d)
par(mfrow=c(1,2))
acf(tser01)
#only correlated at lag equal to zero 
pacf(tser01)
#partial not significant (no partial correlation)
```
The correlogram of the ACF has 1 significant spike at lag 0 before cutting off while none of the spikes are significant on the partial ACF correlogram. This combination leads me to believe the time series is Gaussian white noise (MA(0) process) thus I try fitting an ARIMA(0,0,0) model to the data.


```{r}
#fitting model to estimate parameters
fit_ts1<-arima(tser01,order=c(0,0,0))
print(fit_ts1)
```
The fitted model has a standard deviation of 0.94 close to the expected 1 with the difference due to the model fitting process not being able estimate the parameters and $\sigma$ exactly.

```{r}
#dignostics
tsdiag(fit_ts1)
```
Checking the diagnostics of the model, the residuals don't appear to have any patterns and there is no evidence of autocorrelation in the residuals. The p-values for the Ljung-Box statistic are all consistently high. Therefore there is no evidence of some time structure in the data that the model doesn't capture.

```{r}
lag.plot(resid(fit_ts1),do.lines=FALSE)
```
Viewing the residuals of the ARIMA(0,0,0) fitted model they are a globular smattering of points and thus uncorrelated.

Therefore, I conclude that the time series is an ARIMA(0,0,0) process (white noise process)


#TS 2
```{r}
#Start by plotting the time series
plot(tser02)
```

Initial observation of the second time series object: it doesn't look to have a trend or seasonal component, the variation doesn't appear to change as time increases, thus it appears the time series object is not heteroscedastic.

```{r}
#ks test, spit in half to examine hetroscedasticity
x<-tser02[1:150]
y<-tser02[151:300]
ks.test(x,y)
```
Performing a Kolmogorov-Smirnov test on the data split in half, we get a high p-value of 0.95. This provides evidence the time series is not heteroscedastic and has no trend providing evidence that the process is stationary.

```{r}
#1. Identification of likely p,q,d values
#use correlograms 
par(mfrow=c(1,2))
acf(tser02)
pacf(tser02)
#acf cuts off at 3 and pacf decays exponentially indicating a MA(2) process
```
Viewing the correlagrams the ACF appears to cut off after lag 2 while the partial ACF decays exponentially. This leads me to believe the time series is an ARIMA(0,0,2) process. Using this I try and fit a model to calculate the beta coefficients.

```{r}
#2. Parameter esitmation 
#I fit a model using the the p,q,d values above (00,2) to estimate the parameters (beta)
fit_ts2<- arima(tser02,order=c(0,0,2))
print(fit_ts2)
```
$\sigma$ of the fitted ARIMA(0,0,2) model is estimated to be 1.06 which is close to the expected value of 1
```{r}

#creating 95% CI for beta 1
se_1<- sqrt(fit_ts2$var.coef[1,1])
print(c(fit_ts2$coef[1]-2*se_1,fit_ts2$coef[1]+2*se_1))
#beta 1 significantly different from 0

#beta 2 CI
se_2<- sqrt(fit_ts2$var.coef[2,2])
print(c(fit_ts2$coef[2]-2*se_2,fit_ts2$coef[2]+2*se_2))
#beta significantly different from 0 so 95% sure
```
I then calculate the $\beta_1$ and $\beta_2$ 95% confidence intervals which are displayed above. They are both significantly different from 0.

```{r}
#further dignostics
tsdiag(fit_ts2) 
```
However when creating the diagnostic plots we can see that the Ljung-box statistic of p-values are significant for lags of 2,3,4 and possibly 5. This indicates that there could be some time structure I haven't included in my model that is present in the time series. Thus I will revise my model. After considering the ACF and PACF correlograms again it could be that the ACF is exhibiting sinusoidal dampening and thus as both the ACF and PACF are tailing off the process is ARMA. Thus I try to fit a ARIMA(1,0,1) model to the data.

```{r}
fit_ts2_2<- arima(tser02,order=c(1,0,1))
print(fit_ts2_2)
```
The estimated $\sigma$ is 1.05 which is close to the actual value of 1.

```{r}
#creating 95% CI for alpha_1
se_1 <- sqrt(fit_ts2_2$var.coef[1,1])
print(c(fit_ts2_2$coef[1]-2*se_1,fit_ts2_2$coef[1]+2*se_1))
#alpha significantly different from 0

#CI for beta_1
se_2 <- sqrt(fit_ts2_2$var.coef[2,2])
print(c(fit_ts2_2$coef[2]-2*se_2,fit_ts2_2$coef[2]+2*se_2))
#alpha significantly different from 0
```
Calculating the 95% confidence intervals for $\alpha_1$ and $\beta_1$ gives the above 2 confidence intervals which are both significantly different from 0.

```{r}
tsdiag(fit_ts2_2)
#diagnostics look good: no pattern in residuals, no autocorrelation of residuals and LB p-values all high
```
The diagnostics for this model look better. The residuals appear to have no pattern and the there is no evidence of autocorrelation between the residuals. Furthermore, the p-values for the Ljung-Box statistic are all consistently high (and thus non-significant).
```{r}
#final look at residuals of ARIMA(1,0,1) model
lag.plot(resid(fit_ts2_2),do.lines=FALSE)
```
Residuals are also distributed in a globular pattern for the ARMA(1,1) model

To further investigate if the time series I fit higher order ARMA models to the data to examine if they are preferential to the ARMA(1,1) model fitted above. 

```{r}
#try a p=1, q=2 model
fit_ts2_3<- arima(tser02,order=c(1,0,2))
#creating 95% CI for alpha/beta's
#alpha_1 CI
se_1 <- sqrt(fit_ts2_3$var.coef[1,1])
print(c(fit_ts2_3$coef[1]-2*se_1,fit_ts2_3$coef[1]+2*se_1))
#significantly different from 0

#beta_1 CI
se_2 <- sqrt(fit_ts2_3$var.coef[2,2])
print(c(fit_ts2_3$coef[2]-2*se_2,fit_ts2_3$coef[2]+2*se_2))
#significantly different from 0

#beta_2 CI
se_3 <- sqrt(fit_ts2_3$var.coef[3,3])
print(c(fit_ts2_3$coef[3]-2*se_3,fit_ts2_3$coef[3]+2*se_3))
#not significantly different from 0 as CI includes 0 thus TS shouldn't have q=2
```
Trying to fit a ARMA(1,2) model and creating a 95% confidence intervals for estimated parameters $\alpha_1$, $\beta_1$ and $\beta_2$ gives the above intervals. Importantly the $\beta_2$ confidence interval includes 0 thus it is not significantly different from 0. Therefore the model reduces to an ARMA(1,1) model.

```{r}
# p=2, q=1 model
fit_ts2_4<- arima(tser02,order=c(2,0,1))

#creating CI's
#creating 95% CI for alpha_1
se_1 <- sqrt(fit_ts2_4$var.coef[1,1])
print(c(fit_ts2_4$coef[1]-2*se_1,fit_ts2_4$coef[1]+2*se_1))
#alpha significantly different from 0

#creating 95% CI for alpha_2
se_2 <- sqrt(fit_ts2_4$var.coef[2,2])
print(c(fit_ts2_4$coef[2]-2*se_2,fit_ts2_4$coef[2]+2*se_2))
#alpha 2 not significantly different from 0 thus p=1 not 2 so reject this model
```
Trying to fit a ARMA(2,1) model and creating a 95% confidence interval for $\alpha_1$ and $\alpha_2$ gave the intervals above. Importantly the $\alpha_2$ confidence interval includes 0 so is not significantly different from 0 and therefore I conclude that the this model reduces to the ARMA(1,1) model.



As the models with higher order p or q terms are non significant I conclude that the most suitable model is an ARIMA(1,0,1) is the with $\alpha_1$ estimated to be -0.52 and $\beta_1$ to be -0.61.

##TS 3

```{r}
plot(tser03)
```
Plotting the third time series it does not appear to have a trend or seasonal component, moreover, the variation does not appear to change as time increases therefore the time series appears to not be heteroscedastic.

```{r}
x<-tser03[1:150]
y<-tser03[151:300]
ks.test(x,y)
#no hetroscedasity or trend observed (as p-value high)
```
Performing a Kolmogorov-Smirnov test gives a very high p-value of 0.983 therefore I conclude that the time series is not hetroscedasity and there is no trend, providing evidence that the time series is stationary.

```{r}
#1. Identification of likely p,q,d values
par(mfrow=c(1,2))
#plotting correlograms 
acf(tser03)
#cuts off after lag of 2
pacf(tser03)
#could be decaying however lag increases from 1 to 2 
#initially fit a MA(2) model to the data
```
The acf appears to cut off after the second lag when viewing the correlograms, however I note that the acf could also be interpretted as having a sinusoidal decay while the partial ACF decays with added lag. Therefore I initially attempt fitting an MA(2) model to the data, however I note that if this doesn't explain the time series well (there exists time structures in the data not captured by the model) I will try fitting an ARMA process.

```{r}
#fitting MA(2) model
fit_ts3<- arima(tser03,order=c(0,0,2))
print(fit_ts3)
```

```{r}
#further dignostics
tsdiag(fit_ts3) 
```
Looking at the above diagnostic plots for the ARIMA(0,0,2) model I note that there could be autocorrelation between the residuals (significant spike after 0 lag) and there are significant Ljung-Box statistic p-values for all lags 2 and greater. Thus conclude that there is some time structure/s I have not modeled. 

Thus I try fitting an ARMA(1,1) model as I noted the time series could be an ARMA model above.
```{r}
#try p=1, q=1
fit_ts3_2<- arima(tser03,order=c(1,0,1))
print(fit_ts3_2)
```

```{r}
tsdiag(fit_ts3_2)
```
The model diagnostics still give evidence that there is some time structure not captured by the model fitted as the Ljung-Box statistics p-values are significant for a lag of 2 or greater. Therefore, I revise my model and try to fit an ARIMA(2,0,1) to the time series.


```{r}
#try p=2, q=1
fit_ts3_3<- arima(tser03,order=c(2,0,1))
print(fit_ts3_3)
```
Fitting the model gives an estimated $\sigma$ of 1.02 which is close to the expected value of 1.
```{r}
tsdiag(fit_ts3_3)
#all diagnostics look good (no patterns in residuals or autocorrelation) and LB test has high p-values
```
Checking the diagnostic plots, there if no evidence of any time structure in the time series object not captured by the ARIMA(2,0,1) model as there are no patterns in the residuals, no autocorrelation of the residuals and Ljung-Box statistic p-values are consistently high.

```{r}
#create CI's
#CI for alpha 1
se_1<- sqrt(fit_ts3_3$var.coef[1,1])
print(c(fit_ts3_3$coef[1]-2*se_1,fit_ts3_3$coef[1]+2*se_1))
#alpha 1 not significantly different from 0

#CI for alpha 2
se_2<- sqrt(fit_ts3_3$var.coef[2,2])
print(c(fit_ts3_3$coef[2]-2*se_2,fit_ts3_3$coef[2]+2*se_2))
#alpha 2 significantly different from 0

#CI for beta 1
se_3<- sqrt(fit_ts3_3$var.coef[3,3])
print(c(fit_ts3_3$coef[3]-2*se_3,fit_ts3_3$coef[3]+2*se_3))
#beta 1 significantly different from 0
```
Creating confidence intervals for the parameters $\alpha_1$, $\alpha_2$ and $\beta_1$ estimated by the mdoel gives the intervals above. I note that $\alpha_1$ is not significantly different from 0 but the $\beta_1$ and $\alpha_2$ parameters are significantly different from 0.

```{r}
#now also create a p=2, q=2 model to compare against
fit_ts3_4<-arima(tser03,order=c(2,0,2))
#check if beta_2 term is significant
#CI for beta 2
se_4<- sqrt(fit_ts3_4$var.coef[4,4])
print(c(fit_ts3_4$coef[4]-2*se_4,fit_ts3_4$coef[4]+2*se_4))
#not significantly different from 0 so reject this model
```
Fitting an ARIMA(2,0,2) gave a non-significant $\beta_2$ value (as the 95% confidence interval included 0). Thus this model reduces to an ARIMA(1,0,2) process.

Therefore, I conclude that the time series is an ARIMA(2,0,1) with $\alpha_1=0$, $\alpha_2=-0.31$ and $\beta_1=0.71$,


## TS 4
```{r}
plot(tser04)
```
Looking at a plot of the fourth time series there appears to be no apparent seasonality, trend or heteroscadicity in the time series.

```{r}
x<-tser04[1:150]
y<-tser04[151:300]
ks.test(x,y)
#no hetroscedasity or trend observed (as p-value high)
```
Spliting the time series data in half and performing a Kolmogorov-Smirnov test gives a large non-significant p-value of 0.723 providing evidnce that the time series is not heteroscedastic and does not contain a trend and therefore is stationary.

```{r}
par(mfrow=c(1,2))
#correlograms
acf(tser04)
#cuts off after lag 2
pacf(tser04)
```

Looking at the correlograms of the acf and pacf, both the acf and pacf appear to exhibit sinusoidal dampening thus the model is an ARIMA(p,0,q) model with some value of p and q both greater than 0. 


```{r}
#calculating AIC of each possible combination of p and q for an ARMA process
fit_ts4<- arima(tser04,order=c(1,0,1))
fit_ts4_2<- arima(tser04,order=c(1,0,2))
fit_ts4_3<- arima(tser04,order=c(2,0,1))
fit_ts4_4<- arima(tser04,order=c(2,0,2))

#producing AIC of each model to compare
AIC(fit_ts4)
AIC(fit_ts4_2)
AIC(fit_ts4_3)
AIC(fit_ts4_4)
```

Fitting all possible combinations of p and q ARIMA(p,0,q) models to the data. Then I consider the model selection criteria that the best fitting model is the one with the lowest AIC that exhibits no problems which are defined as no time structures unaccounted for by the model (found by looking at the diagnostic plots) and significant leading order parameters (95% confidence intervals for the leading order p and q parameters do not include 0). Using this criteria I first investigate the lowest AIC model which is the ARIMA(2,0,2) model with a AIC of 814.98.


```{r}
#CI for ARIMA(2,0,2) model

#CI for alpha_1
se_1<- sqrt(fit_ts4_4$var.coef[1,1])
print(c(fit_ts4_4$coef[1]-2*se_1,fit_ts4_4$coef[1]+2*se_1))

#CI for alpha_2
se_2<- sqrt(fit_ts4_4$var.coef[2,2])
print(c(fit_ts4_4$coef[2]-2*se_2,fit_ts4_4$coef[2]+2*se_2))

#CI for beta_1
se_3<- sqrt(fit_ts4_4$var.coef[3,3])
print(c(fit_ts4_4$coef[3]-2*se_3,fit_ts4_4$coef[3]+2*se_3))

#CI for beta_2
se_4<- sqrt(fit_ts4_4$var.coef[4,4])
print(c(fit_ts4_4$coef[4]-2*se_4,fit_ts4_4$coef[4]+2*se_4))
```
Calculating 95% confidence intervals for all 4 parameters in the ARIMA(2,0,2) model seen above, as non include 0 all 4 parameters are significant.

```{r}
tsdiag(fit_ts4_4)
```
All model diagnostics appear good for the ARIMA(2,0,2) model, with no clear patterns in the residuals, no evidence of autocorrelation in the residuals and p-values for all Ljung-Box statistics consistently high thus there is no evidence of a time strucuture in the data not captured by the  model. 

```{r}
lag.plot(resid(fit_ts4_4),do.lines=FALSE)
```
The residual plot of the  ARIMA(2,0,2) model has a globular smattering of residuals indicating white noise of residuals

Therefore as there is no evidence to suggest that the ARIMA(2,0,2) model does not capture the time structure opf the data and as it has the lowest AIC this is my preffered model for this time series. The parameters are estimated to be $\alpha_1=-1.41$, $\alpha_2=-0.71$, $\beta_1=1.13$ and $\beta_2=0.53$.



# TS 5-7

```{r}
#creating some ARIMA models with d=2 to see what processes may look like
arima121 <- arima.sim(n=10000,model=list(order=c(1,2,1),ma=c(0.8),ar=c(0.5)),sd=1)
plot(arima121)

arima221 <- arima.sim(n=10000,model=list(order=c(2,2,1),ma=c(0.6),ar=c(-0.5,-0.3)),sd=1)
plot(arima221)

arima222 <- arima.sim(n=10000,model=list(order=c(2,2,2),ma=c(0.8,0.2),ar=c(0.5,0.3)),sd=2)
plot(arima222)
```
From the above simulations I note that ARIMA processes with $d=2$ when plot appear to be smooth curves, thus for time series plots that appear like this, they are likely ARIMA processes with d=2 and therefore need to be to be differenced twice to get a stationary time series.



#TS 5
```{r}
plot(tser05)
```
Viewing the plot for the fifth time series it is apparent that there is a seasonal and a possible trend component that both need to be removed before trying to fit a model to the data. Looking at the plot there appears to be 6 periods, therefore if 1 period is representing one year there must be 4 observations per month, as there is 288 observations in total and $288=12*6*4$ therefore I update the time series object to have this periodicity.

```{r}
ts5<-ts(tser05[1:288],start=0,end=(6-1/48),frequency=48)
#stl decomposition
plot(stl(ts5,s.window="periodic"))
```
Using stl decomposition to remove the seasonality and trend from the time series gives the above decomposition. Looking at the remainder it appears to now be stationary and not exhibiting any obvious periodicity therefore the periodicity of 48 chosen appears to be correct.

```{r}
plot(diff(tser05,lag=48))
```
After removing the seasonal component of the time series by differencing at a lag of 48 (frequency of 1 period), I plot this transformed time series. The time series still appears to not be stationary.

```{r}
acf(diff(tser05,lag=48))
```
Plotting a correglogram of the acf of the transformed time series it has a slow decrease in autocorrelation with increasing lag which suggests non-stationarity of the transformed time series. Therefore to obtain a stationary time series I difference the time series again at lag 1.

```{r}
plot(diff(diff(tser05,lag=48)))
acf(diff(diff(tser05,lag=48)))
```

The resulting plot of this time series appears to behave like a stationary time series and the acf correlogram now shows the autocorrelation quickly tails off giving evidence that this transformed time series is stationary.

```{r}
#transformed ts
t_ts5<-diff(diff(tser05,lag=48))
x<-t_ts5[1:144]
y<-t_ts5[145:288]
ks.test(x,y)
```
Performing a Kolmogorov-Smirnov test on the transformed time series gives a non-significant p-value of 0.296 giving more evidence that the time series is now stationary after differencing at lag 48 and again at lag 1. Therefore I conclude that this time series is stationary and the original time series is ARIMA process with d=1 and seasonal component with a frequency of 48. I now analysis the stationary component of this time series.

```{r}
#correlograms
par(mfrow=c(1,2))
acf(t_ts5)
pacf(t_ts5)
```

Viewing the correlograms of the transformed stationary time series both the acf and pacf appear to exhibit sinusoidal dampening. Therefore both p and q are greater than 0 as the stationary time series is an ARMA model. I start by trying to fit an ARIMA(1,1,1) model to the time series with seasonality removed (original time series differenced at a lag of 48).

```{r}
#removing seasonal component
t_ts5_2<-diff(tser05,lag=48)
#fitting model
fit_ts5<- arima(t_ts5_2,order=c(1,1,1))
tsdiag(fit_ts5)
```

Fitting an ARIMA(1,1,1) model and producing the above diagnostic plots for the model shows that there is evidence of autocorrelation between residuals and the Ljung-Box statistic p-values are significant for lag 2 and above, therefore there is some time structure/s not captured by the current model in the data.

```{r}
fit_ts5_2<-arima(t_ts5_2,order=c(2,1,1))
tsdiag(fit_ts5_2)
```
Fitting an ARIMA(2,1,1) model, the diagnostic plots still show there is some time structure/s not captured by the this model in the data for same reasons as ARIMA(1,1,1) models diagnostic plots.
```{r}
fit_ts5_3<-arima(t_ts5_2,order=c(1,1,2))
tsdiag(fit_ts5_3)
```
Fitting an ARIMA(1,1,2) model the diagnostic plots now show that there is no clear patterns in the residuals, no evidence of autocorrelation between residuals and the p-values for the Ljung-Box statistic are consistently high. Therefore these diagnostics give no evidence that there is some time structure not captured by this model that is present in the data.

```{r}
#CI for parameters of model
#CI for alpha 1
se_1<- sqrt(fit_ts5_3$var.coef[1,1])
print(c(fit_ts5_3$coef[1]-2*se_1,fit_ts5_3$coef[1]+2*se_1))
#alpha 1 significantly different from 0

#CI for beta 1
se_2<- sqrt(fit_ts5_3$var.coef[2,2])
print(c(fit_ts5_3$coef[2]-2*se_2,fit_ts5_3$coef[2]+2*se_2))
#beta 1 significantly different from 0

#CI for beta 2
se_3<- sqrt(fit_ts5_3$var.coef[3,3])
print(c(fit_ts5_3$coef[3]-2*se_3,fit_ts5_3$coef[3]+2*se_3))
```

Producing 95% confidence intervals for the models estimated parameters, I find that none of them include 0, therefore all 3 parameters are significant. 

```{r}
#AIC of ARIMA()
AIC(fit_ts5_3)
#producing AIC of all higher order models
AIC(arima(t_ts5_2,order=c(2,1,2)))
AIC(arima(t_ts5_2,order=c(2,1,3)))
AIC(arima(t_ts5_2,order=c(3,1,2)))
AIC(arima(t_ts5_2,order=c(3,1,3)))
```

Producing the AIC of all higher order models only the ARIMA(2,1,2) model has a lower AIC compared to the ARIMA(1,1,2) model therefore I investigate this model as a lower AIC indicates it could be a better model for the data.

```{r}
#fitting ARIMA(2,1,2) model
fit_ts5_4<-arima(t_ts5_2,order=c(2,1,2))

#producing CI for alpha_2
#CI for alpha 2
se_2<- sqrt(fit_ts5_4$var.coef[2,2])
print(c(fit_ts5_4$coef[2]-2*se_2,fit_ts5_4$coef[2]+2*se_2))
#beta 1 significantly different from 0
```

After fitting the ARIMA(2,1,2) model to the data the $\alpha_2$ parameter estimate is insignificant as its 95% confidence interval includes zero. Therefore this model is no different to the ARIMA(1,1,2) model.

```{r}
print(fit_ts5_3)
```


Therefore, overall I conclude that this time series is an ARIMA(1,1,2) process with parameters estimated to be $\alpha_1=-0.2$, $\beta_1=-0.77$ and $\beta_2=0.93$ and a seasonal component with frequency equal to 48.


## TS 6
```{r}
plot(tser06)
```

Looking at the plot of time series 6, it appears exhibit the features of a d=2 ARIMA process as it is a smooth curve

```{r}
test<-diff(tser06)
plot(test)
test

plot(diff(tser06))
plot(diff(diff(tser06)))

t_ts6<-diff(diff(tser06))
```

Viewing the plots after taking the first and second order difference it is clear that the time series is not stationary and still has a trend after the taking the first difference but the trend appears to of been eliminated after taking the second order difference. Therefore this backs up my original claims that the time series is an ARIMA process with d=2. Therefore I take the second order difference of the time series and analyse this transformed time series to try and identify the seasonality component of this time series.

```{r}

#take 2 differences to remove trend and use stl to examine which of the possible periods seem most likely 
t_ts6<-diff(diff(tser06))

#1 obs per month
ts6_period<-ts(t_ts6[1:238],start=0,end=(24-3/12),frequency=12)
tmp<-stl(ts6_period,s.window="periodic")
plot(tmp)

comps <- tmp$time.series
plot(comps[,3])
acf(comps[,3],lag.max = 100)

#4 obs per month
ts6_period<-ts(t_ts6[1:238],start=0,end=(5-(3/48)),frequency=48)
tmp<-stl(ts6_period,s.window="periodic")
plot(tmp)

comps <- tmp$time.series
plot(comps[,3])
acf(comps[,3],lag.max = 100)


#5 obs per month
ts6_period<-ts(t_ts6[1:240],start=0,end=(4-(3/60)),frequency=60)
tmp<-stl(ts6_period,s.window="periodic")
plot(tmp)

comps <- tmp$time.series
plot(comps[,3])
acf(comps[,3],lag.max = 100)

#2 obs per month
ts6_period<-ts(tser06[1:240],start=0,end=(10-(3/24)),frequency=24)
tmp<-stl(ts6_period,s.window="periodic")
plot(tmp)

comps <- tmp$time.series
plot(comps[,3])
acf(comps[,3],lag.max = 100)
```

Producing the stl decomposition for each possible frequency of period using the formula $nobs=12*nfreq*nyear$ as there are 240 combinations there are only 4 possible integer combinations of nfreq and n year (integers that make 20: (4,5),(10,2),(5,4),(1,20)) that give more than 1 period. Viewing the above plots the first triplet corresponding to nfreq=1 and nyear=20 appear to fit the data the best as the random component of the stl decomposition appears the stationary and the acf has no large spikes at high lag.

```{r}
#removing the seasonal component found to be 12
t_ts6<-diff(diff(diff(tser06)),lag=12)
acf(t_ts6)

t_ts6<-diff(diff(tser06))
acf(t_ts6)
```



After taking 1st difference the time series is clearly still not stationary but still cannot see any clear signs of periodicity so take another difference and then the time series appears stationary backing up my claim that it is a ARIMA d=2 process.
Therefore to analyse the time series I take 2 differences of lag 1 and produce correlograms for this transformed time series in order to identify p and q.

```{r}
#transformed time series
t_ts6<-diff(diff(tser06))
#producing correlograms
acf(t_ts6,lag.max = 100)
pacf(t_ts6)
plot(t_ts6)
#17 or 18

acf(diff(t_ts6,lag=17),lag.max = 100)
```
Both the acf and partial acf correlograms appear to be decaying sinusoidally thus initially try to fit a ARIMA(1,2,1) model to the original time series data

```{r}
fit_ts6<- arima(tser06,order=c(1,2,1))
tsdiag(fit_ts6)
```
Fitting a ARIMA(1,2,1) model and looking at the diagnostic plots there is evidence of autocorrelation between the residuals and the p-values for teh Ljung-Box statistic are significant for lag 2 and greater. This indicates there is still some time structure in the data that the model isn't accounting for. Therefore I try higher order p and q models.


```{r}
#fitting p-=1 q=2 model
fit_ts6_2<- arima(tser06,order=c(1,2,2))
tsdiag(fit_ts6_2)
```
All diagnostic plots for this fitted model don't show any causes for concern with no pattern to the residuals, no evidence of autocorrelation and consistently high p-values for Ljung-Box statistic.
```{r}
print(fit_ts6_2)
```

The model estimates $\sigma=2.98$ thus is in expected range of 3
```{r}
#look at CI's of parameters
#CI for alpha 1
se_1<- sqrt(fit_ts6_2$var.coef[1,1])
print(c(fit_ts6_2$coef[1]-2*se_1,fit_ts6_2$coef[1]+2*se_1))
#alpha 1 significantly different from 0

#CI for beta 1
se_2<- sqrt(fit_ts6_2$var.coef[2,2])
print(c(fit_ts6_2$coef[2]-2*se_2,fit_ts6_2$coef[2]+2*se_2))
#beta 1 significantly different from 0

#CI for beta 2
se_3<- sqrt(fit_ts6_2$var.coef[3,3])
print(c(fit_ts6_2$coef[3]-2*se_3,fit_ts6_2$coef[3]+2*se_3))
```
Producing 95% confidence intervals for all 3 parameters, I find that all intervals do not include 0 therefore all 3 parameters are significant.
I now produce models with higher orders of p and q to make sure have correct model.

```{r}
#p=2,q=2
fit_ts6_3<- arima(tser06,order=c(2,2,2))
tsdiag(fit_ts6_3)
```
```{r}
#CI for alpha 2
se_2<- sqrt(fit_ts6_3$var.coef[2,2])
print(c(fit_ts6_3$coef[2]-2*se_2,fit_ts6_3$coef[2]+2*se_2))
#alpha 2 significantly different from 0

```
For a fitted ARIMA(2,2,2) model the 95% confidence interval for $\alpha_2$ included zero therefore $\alpha_2$ was not significant and model reduces to ARIMA(1,2,2) as we already have

```{r}
#p=1,q=3
fit_ts6_4<- arima(tser06,order=c(1,2,3))

se_4<- sqrt(fit_ts6_4$var.coef[4,4])
print(c(fit_ts6_4$coef[4]-2*se_4,fit_ts6_4$coef[4]+2*se_4))
```
Fitting a ARIMA(1,2,3) model the $\beta_3$ parameter is not significant as 95% confidence interval for it included 0.

```{r}
fit_ts6_5<- arima(tser06,order=c(3,2,2))

se_3<- sqrt(fit_ts6_5$var.coef[3,3])
print(c(fit_ts6_5$coef[3]-2*se_3,fit_ts6_5$coef[3]+2*se_3))
```
Fitting a ARIMA(3,2,2) model the $\alpha_3$ parameter is not significant as 95% confidence interval included 0.

Therefore as the higher order p and q models have insignificant parameters the ARIMA(1,2,2) model is the best fitting model (as extra parameters no different from zero so shouldn't be included). Therefore this time series is an ARIMA(1,2,2) process with parameters estimated to be $\alpha_1=0.39$, $\beta_1=-0.92$ and $\beta_2=0.74$ and no seasonality.


## TS 7

```{r}
plot(tser07)
```
Viewing the seventh time series there is no visible trend but there is a clear periodicity of 4 periods. Therefore, as the total number of observations is 192 the observations per month is 4 as $192=12*4*4$. Using this I update the time series object to have this periodicity.

```{r}
#adding seasonality to the ts object
ts7<-ts(tser07[1:192],start=0,end=(4-1/48),frequency=48)
#stl decomposition
plot(stl(ts7,s.window="periodic"))
```

Using stl decomposition to remove the seasonality and any trend in the data, looking at the remainder it now appears to no longer be periodic. Therefore I consider the frequency of 48 for one period to be correct.

```{r}
plot(diff(tser07,lag=48))

t_ts7<-diff(tser07,lag = 48)
x<-t_ts7[1:96]
y<-t_ts7[97:192]
ks.test(x,y)
```
Plotting the period with the period removed by differencing at a lag of 48 gives a time series that appears to not have a seasonal component. Performing a Kolmogorov-Smirnov test, splitting the data into two I get a non-significant p-value of 0.87 which gives evidence that the transformed series doesn't have a trend and is stationary. 

```{r}
acf(t_ts7)
```
Looking at the correlogram of the acf for the transformed time series, it appears to dampen sinusoidally which gives evidence the transformed time series is stationary. Therefore there is enough evidence to consider this time series stationary so I now analyses this transformed time series.

```{r}
t_ts7<-diff(tser07,lag = 48)
par(mfrow=c(1,2))
acf(t_ts7)
pacf(t_ts7)
```
From acf correlogram the lag appears to dampen sinusoidally and partial ACF cuts off after lag 2 for the transformed time series this indicates that it is an AR(2) process. Therefore I fit an ARIMA(2,0,0) model to the transformed data.

```{r}
#fitting model
fit_ts7<-arima(t_ts7,order=c(2,0,0))
print(fit_ts7)
```
The ARIMA(2,0,0) model estimated $\sigma$ to be 4.16 which is larger than the expected values, however this is due to the differencing I performed to remove the seasonality which will increase the variance in the data.
```{r}
tsdiag(fit_ts7)
```
All diagnostic plots give no evidence of time structures in the data that are not captured by the model as there is no clear patterns in residuals, no evidence of autocorrelation in the residuals and consistently high p-values for the Ljung-Box statistic.

```{r}
#CI of parameters
#CI for alpha 1
se_1<- sqrt(fit_ts7$var.coef[1,1])
print(c(fit_ts7$coef[1]-2*se_1,fit_ts7$coef[1]+2*se_1))

#CI for alpha 2
se_2<- sqrt(fit_ts7$var.coef[2,2])
print(c(fit_ts7$coef[2]-2*se_2,fit_ts7$coef[2]+2*se_2))
```
Creating the 95% confidence intervals for the parameters estimated by the model, both $\alpha_1$ and $\alpha_2$ are significant as their confidence intervals do not contain 0.

```{r}
#considering higher order models
model_aic<-list()

for (p in 0:3){
  for(q in 0:3){
    model_aic_temp<-model_aic
    model_aic<-append(model_aic_temp,AIC(arima(t_ts7,order=c(p,0,q))))
  }
}
#extracting the AIC's into a vector
test<-unlist(model_aic)
#ordering values from smallest to largest to identify any better models to investigate in detail
order(test)

#order is an ARIMA(3,0,1), ARIMA(2,0,0)

```

After producing all models and ordering them by lowest AIC I find that the ARIMA(3,0,1) model has a lower AIC than ARIMA(2,0,0) (829.9 compared to 831.0). I investigate this model as the lower AIC indicates that this model is a better fit to the data. 

```{r}
fit_ts7_2<-arima(t_ts7,order=c(3,0,1))
print(fit_ts7_2)
```
```{r}
#CI for parameters

#alpha 1 CI
se_1<- sqrt(fit_ts7_2$var.coef[1,1])
print(c(fit_ts7_2$coef[1]-2*se_1,fit_ts7_2$coef[1]+2*se_1))

#alpha 2 CI
se_2<- sqrt(fit_ts7_2$var.coef[2,2])
print(c(fit_ts7_2$coef[2]-2*se_2,fit_ts7_2$coef[2]+2*se_2))

#alpha 3 CI
se_3<- sqrt(fit_ts7_2$var.coef[3,3])
print(c(fit_ts7_2$coef[3]-2*se_3,fit_ts7_2$coef[3]+2*se_3))

#beta  1 CI
se_4<- sqrt(fit_ts7_2$var.coef[4,4])
print(c(fit_ts7_2$coef[4]-2*se_4,fit_ts7_2$coef[4]+2*se_4))

```
All the parameters estimated by the ARIMA(3,0,1) model are significant as none of the intervals include 0.

```{r}
tsdiag(fit_ts7_2)
```
Viewing the diagnostic plots for the ARIMA(3,0,1) model they show no pattern in the residuals, no evidence of autocorrelation between the residuals and the p-values of the Ljung-Box statistic are consistently high giving no evidence of a time structure/s not captured by this model that are present in the data.

```{r}
print(fit_ts7_2)
```

As I found no issues with the fit of the ARIMA(3,0,1) model I choose it over the ARIMA(2,0,0) as it has a lower AIC. Therefore my preferred model for this time series is an ARIMA(3,0,1) model with parameters estimated to be $\alpha_1=-0.18$, $\alpha_2=-0.27$, $\alpha_1=-0.82$ and $\beta_1=1.00$ and an added seasonal component with frequency 48.


# TS 8-10

```{r}
#consider what d=3 processes lookk like
#creating some ARIMA models with d=2 to see what processes may look like
arima131 <- arima.sim(n=10000,model=list(order=c(1,3,1),ma=c(0.8),ar=c(0.5)),sd=1)
plot(arima131)

arima231 <- arima.sim(n=10000,model=list(order=c(2,3,1),ma=c(0.6),ar=c(-0.5,-0.3)),sd=1)
plot(arima231)

arima232 <- arima.sim(n=10000,model=list(order=c(2,3,2),ma=c(0.8,0.2),ar=c(0.5,0.3)),sd=2)
plot(arima232)

```
Producing some simulations of ARIMA processes with d=3, they exhibit smooth curves when plotted.


## TS 8

```{r}
plot(tser08)
```

Looking at the plot for the eigth time series it appears to be none stationary, but does not exhibit any seasonality

```{r}
acf(tser08)
```

Producing the correlogram of the acf of the time series, it decays gradually thus the time series is non-stationary.

```{r}
plot(diff(tser08))
acf(diff(tser08))
```

Therefore I take the 1st difference and produce a plot of the transformed time series and a correlogram of the acf. It appears to be exhibiting sinusoidal damping giving evidence that the transformed time series is no longer stationary and viewing the plot of the transformed time series does not appear to have any obvious trends.

```{r}
t_ts8<-diff(tser08)
x<-t_ts8[1:50]
y<-t_ts8[51:100]
ks.test(x,y)
```
Performing a Kolmogorov-Smirnov test by splitting the transformed time series in half the p-value is non-significant at 0.66 thus I conclude that the transformed time series is stationary and therefore the original time series is an ARIMA process with $d=1$ 

```{r}
#finding p and d
par(mfrow=c(1,2))
acf(t_ts8)
pacf(t_ts8)
```

Viewing the correlograms of the transformed time series to be stationary the acf appears to dampen meanwhile the partial ACF cuts off after the 1st cut-off. Therefore the series appears to be a ARIMA(1,1,0) process

```{r}
#fitting suspected model
fit_ts8<- arima(tser08,order=c(1,1,0))
print(fit_ts8)
tsdiag(fit_ts8)
```
Model diagnostic plots show no clear patterns in the residuals, no autocorrelation between the residuals and p-values for Ljung-Box statistic consistently high (non-significant) thus there is no evidence of a time structure/s not captured by the model that are predsent in the data. I also note that $\sigma=0.97$ thus is close to one one of the expected values of 1.

```{r}
#function to calculate AIC of each model to identitfy any potentially better fitting models
model_aic<-list()
for (p in 0:7){
  for(q in 0:7){
    model_aic_temp<-model_aic
    model_aic<-append(model_aic_temp,AIC(arima(tser08,order=c(p,1,q))))
  }
}
#extracting the AIC's into a vector
test<-unlist(model_aic)
#ordering values from smallest to largest to identify any better models to investigate in detail
order(test)

#10, 18, 11, 17, 25 ar all better than chosen (9) in terms of AIC so need to check their model diagnostics

```

To check the AIC of all models to quickly identify any that have a lower AIC I create a double loop to calculate the AIC of all 64 possible combinations of p and q for an ARIMA model with d=1. I find 5 models that have a lower AIC than the ARIMA(1,1,0) model so need to investigate those models diagnostics as the lower AIC suggests that they provide a better fit to the time series data. 

```{r}
#start with lowest AIC model ARIMA(1,1,1)
fit_ts8_2<-arima(tser08,order=c(1,1,1))
print(fit_ts8_2)
tsdiag(fit_ts8_2)
```
Starting with the model with the lowest AIC, which is an ARIMA(1,1,1) model, the diagnostics plots give no cause for concern with no pattern in the residuals, no evidence of autocorrelation between residuals and all Ljung-Box p-values consistently high. $\sigma$ is estimated to be 0.95 which is close to one of the expected values of 1.

```{r}
#creating CI

#CI for alpha 1
se_1<- sqrt(fit_ts8_2$var.coef[1,1])
print(c(fit_ts8_2$coef[1]-2*se_1,fit_ts8_2$coef[1]+2*se_1))

#CI for beta 1
se_2<- sqrt(fit_ts8_2$var.coef[2,2])
print(c(fit_ts8_2$coef[2]-2*se_2,fit_ts8_2$coef[2]+2*se_2))
```

Creating a 95% confidence interval for the models parameters neither includes 0 so both $\alpha_1$ and $\beta_1$ are significant parameters. Therefore as this model has the lowest AIC and the diagnostics don't give any evidence of time structure/s not captured by the model that are present in the data, the ARIMA(1,1,1) model is my preffered choice for this time series data. The parameters are estimated to be $\alpha_1=-0.86$ and $\beta_1=0.44$

## TS 9

```{r}
plot(tser09)
#appears that it could be a polynomial trend so take difference twice and see if we have a sationary series
```

Plotting the time series I observe it is a smooth curve thus it is likely to be an ARIMA process with d=2 or 3 so I plot transformed time series for taking difference 1, 2 and 3 times

```{r}
par(mfrow=c(3,1))
plot(diff(tser09))
plot(diff(diff(tser09)))
plot(diff(diff(diff(tser09))))
```
Viewing the transformed time series it appears to be ARIMA process with d=2. As after taking difference twice the transformed time series appears stationary.

```{r}
#transforming the time series
t_ts9<-diff(diff(tser09))
acf(t_ts9)
```
Viewing the acf of the transformed time series after taking 2 differences as it cuts off after lag of 0 it gives evidence that the the transformed time series data is stationary, backing up the observation made above.

```{r}
x<-t_ts9[1:50]
y<-t_ts9[51:100]
ks.test(x,y)
```
Performing a Kolmogorov-Smirnov test gives a non significant p-value of 0.184 giving further evidence the transformed time series data is stationary. Thus as all evidence supports the taking the 1st difference twice on the time series data produces a stationary time series, I take the time series data as an ARIMA process with d=2.

```{r}
acf(diff(t_ts9))
```

I also note that the correlogram of the third differenced time series has an lag 1 autocorrelation of -0.5 which is an indication the series may be overdifferenced  as mentioned in (Ming Chun Chang & David A. Dickey, 1994. "Recognizing Overdifferenced Time Series," Journal of Time Series Analysis, Wiley Blackwell, vol. 15(1), pages 1-18, January.)

Therefore I assume the time series is an ARIMA d=2 model so I now analyses the acf and pacf of the second-order differenced time series.

```{r}
acf(t_ts9,lag.max = 50)
pacf(t_ts9,lag.max = 50)
```
The partial acf correlogram of the transformed time series appears to exhibit sinusoidal dampening while the acf correlogram appears to also exhibit sinusoidal damping thus I conclude that both p and q are greater than 0.


```{r,message=F,warning=F}

#change the method to maximum likelihood so that can get better esitmates and don't have none stationarity issue

model_aic<-list()
for (p in 1:7){
  for(q in 1:7){
    model_aic_temp<-model_aic
    model_aic<-append(model_aic_temp,AIC(arima(tser09,order=c(p,2,q),method="ML")))
  }
}
#extracting the AIC's into a vector
test<-unlist(model_aic)
#ordering values from smallest to largest to identify any better models to investigate in detail
order(test)
```

Producing all 49 combinations of the ARIMA model with d=2 and p,q >0 and ordering by lowest AIC I find that the model with the lowest AIC is an ARIMA(4,2,4) model. Thus I investigate this model first as the AIC indicates it is the best fitting model for the time series.

```{r}
fit_ts9<-arima(tser09,order=c(4,2,4))
tsdiag(fit_ts9)
```

The ARIMA(4,2,4) model diagnostic plots show no pattern in the residuals, no autocorrelation between the resdiuals and the p-values of the Ljung-Box statistic are consistently high and therefore non-significant. Therefore they give no evidence that there is a time structure that has not been captured by the model.

```{r}
#CI for the parameters

#alpha 1 CI
se_1<- sqrt(fit_ts9$var.coef[1,1])
print(c(fit_ts9$coef[1]-2*se_1,fit_ts9$coef[1]+2*se_1))

#alpha 2 CI
se_2<- sqrt(fit_ts9$var.coef[2,2])
print(c(fit_ts9$coef[2]-2*se_2,fit_ts9$coef[2]+2*se_2))

#alpha 3 CI
se_3<- sqrt(fit_ts9$var.coef[3,3])
print(c(fit_ts9$coef[3]-2*se_3,fit_ts9$coef[3]+2*se_3))

#alpha 4 1 CI
se_4<- sqrt(fit_ts9$var.coef[4,4])
print(c(fit_ts9$coef[4]-2*se_4,fit_ts9$coef[4]+2*se_4))

#beta 1 CI
se_5<- sqrt(fit_ts9$var.coef[5,5])
print(c(fit_ts9$coef[5]-2*se_5,fit_ts9$coef[5]+2*se_5))

#beta 2 CI
se_6<- sqrt(fit_ts9$var.coef[6,6])
print(c(fit_ts9$coef[6]-2*se_6,fit_ts9$coef[6]+2*se_6))

#beta 3 CI
se_7<- sqrt(fit_ts9$var.coef[7,7])
print(c(fit_ts9$coef[7]-2*se_7,fit_ts9$coef[7]+2*se_7))

#beta 4 CI
se_8<- sqrt(fit_ts9$var.coef[8,8])
print(c(fit_ts9$coef[8]-2*se_8,fit_ts9$coef[8]+2*se_8))
```
Creating 95% CI for the parameter estimated of the ARIMA(4,2,4) model I find that all parameters are significant as none of the calculated intervals include 0.

```{r}
print(fit_ts9)
```
The estimated $\sigma$ is 2.85 which is close to 3 

Overall there is no reason to reject the ARIMA(4,2,4) model and as this model has the lowest AIC of all possible models using that as a selection criteria for the best fitting model my preferred model for this time series is an ARIMA(4,2,4) process with parameters estimated to be $\alpha_1=-0.37$,$\alpha_2=-0.33$ , $\alpha_3=0.48$,$\alpha_4=-0.76$,$\beta_1=-0.49$, $\beta_2=0.77$, $\beta_3=-0.49$ and $\beta_4=1$



## TS 10

```{r}
plot(tser10)
```
Plotting the tenth time series it appears it could be non stationary.

```{r}
x<-tser10[1:50]
y<-tser10[51:100]
ks.test(x,y)
```

Performing a Kolmogorov-Smirnov test gives a non-significant p-value of 0.1124 giving evidence that the time series is stationary and theredfore it is an ARIMA process with therefore d=0.


```{r}
acf(tser10)
pacf(tser10)
```
The acf appears to exhibit sinusoidal decay giving more evidence that the time series is stationary. Due to the Kolmogorov-Smirnov test and acf shape I conclude that the time series is stationary. The pacf of the time series cuts off after a lag of 2 thus as the acf tails off there is evidence that the time series is an ARIMA(2,0,0)


```{r,warning=F}
model_aic<-list()
for (p in 0:7){
  for(q in 0:7){
    model_aic_temp<-model_aic
    model_aic<-append(model_aic_temp,AIC(arima(tser10,order=c(p,0,q))))
  }
}
#extracting the AIC's into a vector
test<-unlist(model_aic)
#ordering values from smallest to largest to identify any better models to investigate in detail
order(test)
```

Fitting all models and ordering them from smallest AIC the ARIMA(2,0,0) model has the lowest AIC.

```{r}
#ARIMA(2,0,0) corresponds to 17th entry of loop (16th is ARIMA(1,0,7))
fit_ts10<-arima(tser10,order=c(2,0,0))
tsdiag(fit_ts10)
```
Investigating the diagnostic plots of the ARIMA(2,0,0) model there are no patterns in the residuals, no evidence of autocorrelation between the residuals and the Ljung-box p-values are all consistently high. Therefore there is no evidence of a time structure/s not explained by the model.

```{r}
#calculating CI's for parameters

#alpha 1 CI
se_1<- sqrt(fit_ts10$var.coef[1,1])
print(c(fit_ts10$coef[1]-2*se_1,fit_ts10$coef[1]+2*se_1))

#alpha 2 CI
se_2<- sqrt(fit_ts10$var.coef[2,2])
print(c(fit_ts10$coef[2]-2*se_2,fit_ts10$coef[2]+2*se_2))
```
The 95% confidence interval of both $\alpha_1$ and $\alpha_2$ don't include 0 and are therefore both significant.

```{r}
print(fit_ts10)
```
The estimate $\sigma$ of the model is 2.84 which is close to an expected possible value of 3.

As the ARIMA(2,0,0) model has the lowest AIC of all possible combinations of p and q and diagnostics give no evidence that there is any time structure not explained by the model, the ARIMA(2,0,0) model with parameters estimated to be $\alpha_1=-1.77$ and $\alpha_2=-0.89$ is my preffered model for this time series.
























